{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b7f3136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta/llama-4-scout-17b-16e-instruct\n",
    "\n",
    "\n",
    "import IPython\n",
    "import requests\n",
    "\n",
    "image_url = \"https://www.nvidia.com/content/dam/en-zz/Solutions/research/ai-playground/nvidia-picasso-3c33-p@2x.jpg\"  ## Large Image\n",
    "image_url = \"https://www.nasa.gov/wp-content/uploads/2025/04/ksc-20200622-ph-jbs01-0070orig.jpg?w=2048\"  ## Large Image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/1/1c/2019_Rolls-Royce_Phantom_V12_Automatic_6.75.jpg\"  ## Large Image\n",
    "image_content = requests.get(image_url).content\n",
    "\n",
    "IPython.display.Image(image_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede1390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learn_langgraph\\.venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:212: UserWarning: Found meta/llama-4-scout-17b-16e-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "meta/llama-3.1-405b-instruct\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-4-scout-17b-16e-instruct\",api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "629db224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learn_langgraph\\.venv\\Lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:176: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sample = ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4409cb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(id='ai21labs/jamba-1.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, base_model=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sample[0])  # 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af191b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[2].id#'meta/llama-4-scout-17b-16e-instruct'\n",
    "supported = []\n",
    "for trash in sample:\n",
    "    if trash.supports_tools == True:\n",
    "        supported+= [trash.id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fba8539d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mistralai/mistral-large-2-instruct',\n",
       " 'meta/llama-3.3-70b-instruct',\n",
       " 'meta/llama-3.1-405b-instruct',\n",
       " 'meta/llama-3.1-70b-instruct',\n",
       " 'meta/llama-3.1-8b-instruct',\n",
       " 'meta/llama-3.2-3b-instruct',\n",
       " 'nv-mistralai/mistral-nemo-12b-instruct']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9dfd67a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.1-405b-instruct\",api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7200d659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm a large language model, I have a massive number of parameters!\\n\\nTo be more specific, I am a transformer-based model, and my architecture is based on the popular BERT (Bidirectional Encoder Representations from Transformers) model.\\n\\nMy model has approximately **175 billion parameters**. Yes, you read that right, 175 billion!\\n\\nThese parameters are distributed across multiple layers, including:\\n\\n1. Embedding layers: 40 million parameters\\n2. Encoder layers: 120 billion parameters (across 24 layers)\\n3. Decoder layers: 15 billion parameters (across 6 layers)\\n4. Other layers (e.g., pooling, normalization): 100 million parameters\\n\\nThese parameters are learned during training and allow me to understand and generate human-like language. However, it's worth noting that having many parameters doesn't necessarily mean I'm more intelligent or capable than other models. There are many other factors that contribute to a model's performance, such as training data, architecture design, and optimization techniques.\", additional_kwargs={}, response_metadata={'role': 'assistant', 'content': \"I'm a large language model, I have a massive number of parameters!\\n\\nTo be more specific, I am a transformer-based model, and my architecture is based on the popular BERT (Bidirectional Encoder Representations from Transformers) model.\\n\\nMy model has approximately **175 billion parameters**. Yes, you read that right, 175 billion!\\n\\nThese parameters are distributed across multiple layers, including:\\n\\n1. Embedding layers: 40 million parameters\\n2. Encoder layers: 120 billion parameters (across 24 layers)\\n3. Decoder layers: 15 billion parameters (across 6 layers)\\n4. Other layers (e.g., pooling, normalization): 100 million parameters\\n\\nThese parameters are learned during training and allow me to understand and generate human-like language. However, it's worth noting that having many parameters doesn't necessarily mean I'm more intelligent or capable than other models. There are many other factors that contribute to a model's performance, such as training data, architecture design, and optimization techniques.\", 'token_usage': {'prompt_tokens': 17, 'total_tokens': 222, 'completion_tokens': 205}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-405b-instruct'}, id='run-de72dd2e-d91f-4419-a9f4-7ff8abcbc194-0', usage_metadata={'input_tokens': 17, 'output_tokens': 205, 'total_tokens': 222}, role='assistant')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how many parameter you have ?\")  # Initialize tools for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c564c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The license plate on the car reads R30 GWD.', additional_kwargs={}, response_metadata={'role': 'assistant', 'reasoning_content': None, 'content': 'The license plate on the car reads R30 GWD.', 'tool_calls': [], 'token_usage': {'prompt_tokens': 2340, 'total_tokens': 2353, 'completion_tokens': 13, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'meta/llama-4-scout-17b-16e-instruct'}, id='run-0a1ba360-3fcd-4502-8a5c-2e6c414dc6fa-0', usage_metadata={'input_tokens': 2340, 'output_tokens': 13, 'total_tokens': 2353}, role='assistant')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=[\n",
    "                {\"type\": \"text\", \"text\": \"what is this number of the car?\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "                # {\"type\": \"image_content\", \"image_content\": {\"image_content\":image_content}},\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c9bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
